{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644dff9",
   "metadata": {},
   "source": [
    "Import Library and create a client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16385529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# ðŸ”¹ For LM Studio (local)\n",
    "lmstudio_client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",  # LM Studio local API\n",
    "    api_key=\"lm-studio\"  # dummy key required by library\n",
    ")\n",
    "\n",
    "# ðŸ”¹ For OpenAI cloud (official API)\n",
    "# cloud_client = OpenAI(api_key=\"your-real-openai-key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520293c3",
   "metadata": {},
   "source": [
    "Prompt Engineering is the practice of designing and refining inputs (prompts) given to a Large Language Model (LLM) to achieve the desired output. Since LLMs donâ€™t inherently â€œunderstandâ€ like humans, the way you phrase your instructions, examples, and context can drastically affect the quality, accuracy, and reliability of the responses.\n",
    "\n",
    "Itâ€™s a mix of art (clear communication) and science (using structured methods).\n",
    "\n",
    "Zero-Shot Prompting\n",
    "Definition: Asking the model to perform a task without giving examples.\n",
    "Use case: When you want the model to generalize directly from the instruction.\n",
    "Example:\n",
    "\"Translate 'How are you?' into Spanish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ed2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt(prompt, client, model=\"gemma-3-12b-it\"):\n",
    "    \"\"\"\n",
    "    Send a zero-shot prompt using either LM Studio or OpenAI cloud.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c652d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example zero-shot test (LM Studio)\n",
    "resp = zero_shot_prompt(\n",
    "    \"Translate 'Good morning, how are you?' into French.\",\n",
    "    lmstudio_client,\n",
    "    model=\"gemma-3-12b-it\"  # match LM Studio model name\n",
    ")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821243e",
   "metadata": {},
   "source": [
    "Few-Shot Prompting\n",
    "Definition: Provide multiple examples to set a pattern before asking the model to complete a task.\n",
    "Use case: When you need the model to mimic a style, format, or reasoning pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(examples, query, client, model=\"gemma-3-12b-it\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    \n",
    "    # Add few-shot examples as user/assistant turns\n",
    "    for ex in examples:\n",
    "        messages.append({\"role\": \"user\", \"content\": ex[\"question\"]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ex[\"answer\"]})\n",
    "    \n",
    "    # Add the actual query at the end\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot examples\n",
    "examples = [\n",
    "    {\"question\": \"Translate 'Good morning' into Spanish.\", \"answer\": \"Buenos dÃ­as.\"},\n",
    "    {\"question\": \"Translate 'How are you?' into Spanish.\", \"answer\": \"Â¿CÃ³mo estÃ¡s?\"}\n",
    "]\n",
    "\n",
    "# Real query\n",
    "query = \"Translate 'See you later' into Spanish.\"\n",
    "\n",
    "# Run with LM Studio client\n",
    "resp = few_shot_prompt(examples, query, lmstudio_client, model=\"your-model-name\")\n",
    "print(\"Response:\", resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ec21e",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) Prompting\n",
    "Definition: Encourage the model to show its reasoning step by step before giving the final answer.\n",
    "Use case: Useful for math, logic, reasoning-heavy tasks.\n",
    "Example:\n",
    "\"Solve: 36 Ã· 12 Ã— 3 + 1. Show your reasoning step by step before giving the final answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7004fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cot_prompt(prompt, client, model=\"gemma-3-12b-it\", reasoning_instruction=True):\n",
    "    system_message = \"You are a helpful assistant.\"\n",
    "    if reasoning_instruction:\n",
    "        # Encourage step-by-step reasoning\n",
    "        user_message = (\n",
    "            f\"{prompt}\\n\\n\"\n",
    "            \"Please think step by step before giving the final answer.\"\n",
    "        )\n",
    "    else:\n",
    "        user_message = prompt\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math reasoning example\n",
    "query = \"If there are 3 cars and each car has 4 wheels, how many wheels are there in total?\"\n",
    "\n",
    "resp = cot_prompt(query, lmstudio_client, model=\"your-model-name\")\n",
    "print(\"Response:\\n\", resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6289193",
   "metadata": {},
   "source": [
    "Instruction Prompting\n",
    "Definition: Clearly state instructions in a directive form.\n",
    "Use case: Useful for coding, structured outputs, summaries, etc.\n",
    "Example:\n",
    "\"Write a Python function that reverses a string. Return only the code without explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f05092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_prompt(instruction, client, model=\"gemma-3-12b-it\"):\n",
    "    \"\"\"\n",
    "    Run an instruction-style prompt (direct task prompting).\n",
    "\n",
    "    Parameters:\n",
    "        instruction (str): The task or instruction for the model\n",
    "        client (OpenAI): OpenAI-compatible client (LM Studio or OpenAI)\n",
    "        model (str): Model name (e.g., \"gpt-4o-mini\" or LM Studio model)\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that follows instructions carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Summarization\n",
    "resp1 = instruction_prompt(\n",
    "    \"Summarize the following text in one sentence:\\n\\nArtificial Intelligence is rapidly advancing and being applied in healthcare, education, and finance.\",\n",
    "    lmstudio_client,\n",
    "    model=\"your-model-name\"\n",
    ")\n",
    "print(\"Summarization:\\n\", resp1)\n",
    "\n",
    "# Example 2: Code generation\n",
    "resp2 = instruction_prompt(\n",
    "    \"Write a Python function that calculates the factorial of a number.\",\n",
    "    lmstudio_client,\n",
    "    model=\"your-model-name\"\n",
    ")\n",
    "print(\"Python Code:\\n\", resp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357e0f3",
   "metadata": {},
   "source": [
    "What is LangChain?\n",
    "\n",
    "LangChain is an open-source framework designed to make it easier to build applications powered by Large Language Models (LLMs) like GPT, Claude, or open-source models (via LM Studio, HuggingFace, etc.).\n",
    "\n",
    "Instead of just sending a prompt â†’ getting a response, LangChain provides tools, abstractions, and integrations that let you:\n",
    "\n",
    "Manage prompts (prompt templates, chaining prompts together).\n",
    "\n",
    "Connect LLMs to external data (databases, APIs, PDFs, websites).\n",
    "\n",
    "Enable memory (so your chatbot remembers past conversations).\n",
    "\n",
    "Use reasoning workflows (e.g., break down a task step by step).\n",
    "\n",
    "Orchestrate agents (let the chatbot call tools or functions when needed).\n",
    "\n",
    "Think of LangChain as the glue that connects an LLM to real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42582292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0caad302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # âœ… correct import\n",
    "\n",
    "def create_langchain_chat(model=\"gemma-3-12b-it\", use_lmstudio=True, api_key=None):\n",
    "    \n",
    "    # Setup model\n",
    "    if use_lmstudio:\n",
    "        llm = ChatOpenAI(\n",
    "            openai_api_base=\"http://100.65.86.31:1234/v1/\",\n",
    "            openai_api_key=\"lm-studio\",  # dummy key\n",
    "            model=model\n",
    "        )\n",
    "    else:\n",
    "        llm = ChatOpenAI(model=model, openai_api_key=api_key)\n",
    "\n",
    "    # Prompt template with memory slot\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that remembers context.\"),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Store conversation histories per session\n",
    "    store = {}\n",
    "    def get_history(session_id: str):\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "    chat = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        get_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"history\"\n",
    "    )\n",
    "\n",
    "    def chat_fn(message: str, session_id=\"default\"):\n",
    "        resp = chat.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        return resp.content\n",
    "\n",
    "    return chat_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85498a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat function for LM Studio\n",
    "chat = create_langchain_chat(model=\"your-model-name\", use_lmstudio=True)\n",
    "\n",
    "print(chat(\"Hello! Who won the 2018 FIFA World Cup?\", session_id=\"user1\"))\n",
    "print(chat(\"And who was the top scorer?\", session_id=\"user1\"))\n",
    "print(chat(\"Summarize what we talked about.\", session_id=\"user1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a50e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = create_langchain_chat(model=\"your-model-name\", use_lmstudio=True)\n",
    "while True:\n",
    "    session_id = 'user1'\n",
    "    query = input('Chat: ')\n",
    "    response = chat(query,session_id)\n",
    "    print(response, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb284194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "def load_pdf_documents(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# def load_codebase(directory: str, extensions=[\".py\", \".ino\"]) -> list:\n",
    "#     documents = []\n",
    "#     for root, _, files in os.walk(directory):\n",
    "#         for file in files:\n",
    "#             if any(file.endswith(ext) for ext in extensions):\n",
    "#                 path = os.path.join(root, file)\n",
    "#                 loader = TextLoader(path)\n",
    "#                 documents.extend(loader.load())\n",
    "#     return documents\n",
    "\n",
    "# def split_documents(documents):\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=600,\n",
    "#         chunk_overlap=60\n",
    "#     )\n",
    "#     return splitter.split_documents(documents)\n",
    "\n",
    "def split_documents_v1(documents):\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_base=\"http://100.65.86.31:1234/v1/\",\n",
    "        openai_api_key=\"lm-studio\",\n",
    "        check_embedding_ctx_length=False\n",
    "    )\n",
    "    splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\"  # Optional: could be \"standard_deviation\" too\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# def split_code_documents(documents):\n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=800,\n",
    "#         chunk_overlap=100,\n",
    "#         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], \n",
    "#         length_function=len\n",
    "#     )\n",
    "#     return splitter.split_documents(documents)\n",
    "\n",
    "def create_db(docs, db_file):\n",
    "    embedding = OpenAIEmbeddings(\n",
    "        openai_api_base=\"http://100.65.86.31:1234/v1/\",\n",
    "        api_key=\"lm-studio\",\n",
    "        check_embedding_ctx_length=False\n",
    "    )\n",
    "    vector_store = FAISS.from_documents(docs, embedding=embedding)\n",
    "\n",
    "    # Save the vector store to a folder\n",
    "    vector_store.save_local(db_file)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "def create_or_append_db(docs, db_file):\n",
    "    embedding = OpenAIEmbeddings(\n",
    "        openai_api_base=\"http://100.65.86.31:1234/v1/\",\n",
    "        api_key=\"lm-studio\",\n",
    "        check_embedding_ctx_length=False\n",
    "    )\n",
    "    if os.path.exists(db_file):\n",
    "        print(\"Appending to existing vector store...\")\n",
    "        vector_store = FAISS.load_local(db_file, embedding, allow_dangerous_deserialization=True)\n",
    "        vector_store.add_documents(docs)\n",
    "    else:\n",
    "        print(\"Creating a new vector store...\")\n",
    "        vector_store = FAISS.from_documents(docs, embedding=embedding)\n",
    "    \n",
    "    vector_store.save_local(db_file)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def load_db(db_file):\n",
    "    # Load the vector store from a folder\n",
    "    embedding = OpenAIEmbeddings(\n",
    "        openai_api_base=\"http://100.65.86.31:1234/v1/\",\n",
    "        api_key=\"lm-studio\",\n",
    "        check_embedding_ctx_length=False\n",
    "    )\n",
    "    return FAISS.load_local(db_file, embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "def create_chain(vector_store:FAISS, event:str = None):\n",
    "    model = ChatOpenAI(base_url=\"http://100.65.86.31:1234/v1/\", api_key=\"lm-studio\", temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "        \"You are an good assistant that handles documents.\\n\"\n",
    "        \"Context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "    ])\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        prompt=retriever_prompt\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(history_aware_retriever, chain)\n",
    "    return retrieval_chain\n",
    "\n",
    "def process_chat(chain, question, chat_history):\n",
    "    response = chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return response[\"answer\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
