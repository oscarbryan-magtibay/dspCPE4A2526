{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644dff9",
   "metadata": {},
   "source": [
    "Import Library and create a client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16385529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# üîπ For LM Studio (local)\n",
    "lmstudio_client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",  # LM Studio local API\n",
    "    api_key=\"lm-studio\"  # dummy key required by library\n",
    ")\n",
    "\n",
    "# üîπ For OpenAI cloud (official API)\n",
    "# cloud_client = OpenAI(api_key=\"your-real-openai-key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520293c3",
   "metadata": {},
   "source": [
    "Prompt Engineering is the practice of designing and refining inputs (prompts) given to a Large Language Model (LLM) to achieve the desired output. Since LLMs don‚Äôt inherently ‚Äúunderstand‚Äù like humans, the way you phrase your instructions, examples, and context can drastically affect the quality, accuracy, and reliability of the responses.\n",
    "\n",
    "It‚Äôs a mix of art (clear communication) and science (using structured methods).\n",
    "\n",
    "Zero-Shot Prompting\n",
    "Definition: Asking the model to perform a task without giving examples.\n",
    "Use case: When you want the model to generalize directly from the instruction.\n",
    "Example:\n",
    "\"Translate 'How are you?' into Spanish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ed2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt(prompt, client, model=\"gemma-3-12b-it\"):\n",
    "    \"\"\"\n",
    "    Send a zero-shot prompt using either LM Studio or OpenAI cloud.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c652d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example zero-shot test (LM Studio)\n",
    "resp = zero_shot_prompt(\n",
    "    \"Translate 'Good morning, how are you?' into French.\",\n",
    "    lmstudio_client,\n",
    "    model=\"gemma-3-12b-it\"  # match LM Studio model name\n",
    ")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821243e",
   "metadata": {},
   "source": [
    "Few-Shot Prompting\n",
    "Definition: Provide multiple examples to set a pattern before asking the model to complete a task.\n",
    "Use case: When you need the model to mimic a style, format, or reasoning pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(examples, query, client, model=\"gemma-3-12b-it\"):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    \n",
    "    # Add few-shot examples as user/assistant turns\n",
    "    for ex in examples:\n",
    "        messages.append({\"role\": \"user\", \"content\": ex[\"question\"]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ex[\"answer\"]})\n",
    "    \n",
    "    # Add the actual query at the end\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot examples\n",
    "examples = [\n",
    "    {\"question\": \"Translate 'Good morning' into Spanish.\", \"answer\": \"Buenos d√≠as.\"},\n",
    "    {\"question\": \"Translate 'How are you?' into Spanish.\", \"answer\": \"¬øC√≥mo est√°s?\"}\n",
    "]\n",
    "\n",
    "# Real query\n",
    "query = \"Translate 'See you later' into Spanish.\"\n",
    "\n",
    "# Run with LM Studio client\n",
    "resp = few_shot_prompt(examples, query, lmstudio_client, model=\"your-model-name\")\n",
    "print(\"Response:\", resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ec21e",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) Prompting\n",
    "Definition: Encourage the model to show its reasoning step by step before giving the final answer.\n",
    "Use case: Useful for math, logic, reasoning-heavy tasks.\n",
    "Example:\n",
    "\"Solve: 36 √∑ 12 √ó 3 + 1. Show your reasoning step by step before giving the final answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7004fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cot_prompt(prompt, client, model=\"gemma-3-12b-it\", reasoning_instruction=True):\n",
    "    system_message = \"You are a helpful assistant.\"\n",
    "    if reasoning_instruction:\n",
    "        # Encourage step-by-step reasoning\n",
    "        user_message = (\n",
    "            f\"{prompt}\\n\\n\"\n",
    "            \"Please think step by step before giving the final answer.\"\n",
    "        )\n",
    "    else:\n",
    "        user_message = prompt\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math reasoning example\n",
    "query = \"If there are 3 cars and each car has 4 wheels, how many wheels are there in total?\"\n",
    "\n",
    "resp = cot_prompt(query, lmstudio_client, model=\"your-model-name\")\n",
    "print(\"Response:\\n\", resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6289193",
   "metadata": {},
   "source": [
    "Instruction Prompting\n",
    "Definition: Clearly state instructions in a directive form.\n",
    "Use case: Useful for coding, structured outputs, summaries, etc.\n",
    "Example:\n",
    "\"Write a Python function that reverses a string. Return only the code without explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f05092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_prompt(instruction, client, model=\"gemma-3-12b-it\"):\n",
    "    \"\"\"\n",
    "    Run an instruction-style prompt (direct task prompting).\n",
    "\n",
    "    Parameters:\n",
    "        instruction (str): The task or instruction for the model\n",
    "        client (OpenAI): OpenAI-compatible client (LM Studio or OpenAI)\n",
    "        model (str): Model name (e.g., \"gpt-4o-mini\" or LM Studio model)\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that follows instructions carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Summarization\n",
    "resp1 = instruction_prompt(\n",
    "    \"Summarize the following text in one sentence:\\n\\nArtificial Intelligence is rapidly advancing and being applied in healthcare, education, and finance.\",\n",
    "    lmstudio_client,\n",
    "    model=\"your-model-name\"\n",
    ")\n",
    "print(\"Summarization:\\n\", resp1)\n",
    "\n",
    "# Example 2: Code generation\n",
    "resp2 = instruction_prompt(\n",
    "    \"Write a Python function that calculates the factorial of a number.\",\n",
    "    lmstudio_client,\n",
    "    model=\"your-model-name\"\n",
    ")\n",
    "print(\"Python Code:\\n\", resp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357e0f3",
   "metadata": {},
   "source": [
    "What is LangChain?\n",
    "\n",
    "LangChain is an open-source framework designed to make it easier to build applications powered by Large Language Models (LLMs) like GPT, Claude, or open-source models (via LM Studio, HuggingFace, etc.).\n",
    "\n",
    "Instead of just sending a prompt ‚Üí getting a response, LangChain provides tools, abstractions, and integrations that let you:\n",
    "\n",
    "Manage prompts (prompt templates, chaining prompts together).\n",
    "\n",
    "Connect LLMs to external data (databases, APIs, PDFs, websites).\n",
    "\n",
    "Enable memory (so your chatbot remembers past conversations).\n",
    "\n",
    "Use reasoning workflows (e.g., break down a task step by step).\n",
    "\n",
    "Orchestrate agents (let the chatbot call tools or functions when needed).\n",
    "\n",
    "Think of LangChain as the glue that connects an LLM to real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42582292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-openai -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0caad302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # ‚úÖ correct import\n",
    "\n",
    "def create_langchain_chat(model=\"gemma-3-12b-it\", use_lmstudio=True, api_key=None):\n",
    "    \n",
    "    # Setup model\n",
    "    if use_lmstudio:\n",
    "        llm = ChatOpenAI(\n",
    "            openai_api_base=\"http://localhost:1234/v1\",\n",
    "            openai_api_key=\"lm-studio\",  # dummy key\n",
    "            model=model\n",
    "        )\n",
    "    else:\n",
    "        llm = ChatOpenAI(model=model, openai_api_key=api_key)\n",
    "\n",
    "    # Prompt template with memory slot\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that remembers context.\"),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Store conversation histories per session\n",
    "    store = {}\n",
    "    def get_history(session_id: str):\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "    chat = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        get_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"history\"\n",
    "    )\n",
    "\n",
    "    def chat_fn(message: str, session_id=\"default\"):\n",
    "        resp = chat.invoke(\n",
    "            {\"input\": message},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        return resp.content\n",
    "\n",
    "    return chat_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d85498a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! France won the 2018 FIFA World Cup! üèÜ\n",
      "\n",
      "\n",
      "\n",
      "Is there anything else I can help you with today?\n",
      "The top scorer of the 2018 FIFA World Cup was Harry Kane from England! He scored six goals throughout the tournament. ‚öΩÔ∏è\n",
      "\n",
      "\n",
      "\n",
      "Anything else you'd like to know about the World Cup or something different entirely?\n",
      "Certainly! Here‚Äôs a summary of our conversation:\n",
      "\n",
      "We discussed the 2018 FIFA World Cup. I told you that **France won** and you then asked who the top scorer was. The top scorer for that year was **Harry Kane from England**, with six goals.\n",
      "\n",
      "\n",
      "\n",
      "Is there anything else I can help clarify or discuss?\n"
     ]
    }
   ],
   "source": [
    "# Create chat function for LM Studio\n",
    "chat = create_langchain_chat(model=\"your-model-name\", use_lmstudio=True)\n",
    "\n",
    "print(chat(\"Hello! Who won the 2018 FIFA World Cup?\", session_id=\"user1\"))\n",
    "print(chat(\"And who was the top scorer?\", session_id=\"user1\"))\n",
    "print(chat(\"Summarize what we talked about.\", session_id=\"user1\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
